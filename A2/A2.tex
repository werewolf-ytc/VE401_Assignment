
  
\documentclass[12pt,a4paper]{article}
\usepackage{amsmath,amscd,amsbsy,amssymb,latexsym,url,bm,amsthm}
\usepackage{epsfig,graphicx,subfigure}
\usepackage{enumitem,balance}
\usepackage{wrapfig}
\usepackage{mathrsfs,euscript}
\usepackage[usenames]{xcolor}
\usepackage{hyperref}
\usepackage[vlined,ruled,commentsnumbered,linesnumbered]{algorithm2e}

%note: use \setcounter{exercise}{5} to change the numbering
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}
\newtheorem*{solution}{Solution}

%copied from VE281. I don't know what they are used for. Some are used to expand the margin.
\setlength{\oddsidemargin}{-0.365in}
\setlength{\evensidemargin}{-0.365in}
\setlength{\topmargin}{-0.3in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textheight}{10.1in}
\setlength{\textwidth}{7in}
\makeatletter \renewenvironment{proof}[1][Proof] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother
\makeatletter
\renewenvironment{solution}[1][Solution] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother

\begin{document}
\title{VE401 Assignment 2}
\author{Yang, Tiancheng 517370910259\\Qiu, Yuqing 518370910026\\Chang, Siyao 517370910024}

\maketitle

\newpage

\begin{exercise}
Discrete Uniform Distribution
\begin{enumerate}[label=\roman*)]
    \item 
    \begin{solution}
            With the parameter $n$, we have
            \begin{equation*}
                \begin{split}
                m_x(t)&=E(e^{xt})=\sum_{k=1}^{n} e^{x_k t}\frac{1}{n}\\
                &=\frac{1}{n}\sum_{k=1}^{n} e^{x_k t}
                \end{split}
            \end{equation*}
    \end{solution}
    \item \begin{solution}
        From the moment generating function we get that
        \begin{equation*}
            \begin{split}
            E[X]&=\frac{d}{dt}m_x(t)|_{t=0}\\
            &=\frac{1}{n}\sum_{k=1}^n [\frac{d}{dt}e^{x_k t}|_{t=0}]\\
            &=\frac{1}{n}\sum_{k=1}^n x_k
            \end{split}
        \end{equation*}
        and
        \begin{equation*}
            \begin{split}
            E[X^2]&=\frac{d^2}{dt^2}m_x(t)|_{t=0}\\
            &=\frac{1}{n}\sum_{k=1}^n[\frac{d^2}{dt^2}e^{x_k t}|_{t=0}]\\
            &=\frac{1}{n}\sum_{k=1}^n x_k^2
            \end{split}
        \end{equation*}
        And hence the variance is given by
        \begin{equation*}
            Var[X]=E[X^2]-E[X]^2=\frac{1}{n}\sum_{k=1}^n x_k^2-\frac{1}{n^2}(\sum_{k=1}^n x_k)^2
        \end{equation*}
    \end{solution}
\end{enumerate}
\end{exercise}

\begin{exercise}
    Uniqueness of Moment generating functions - Simple Case
    \begin{proof}
        With $m_X(t)=m_Y(t)\ \forall t\in (-\varepsilon,\varepsilon)$ we can see that
        \begin{equation*}
            \frac{d}{dt}m_X(t)=\frac{d}{dt}m_Y(t) \ \forall t\in (-\varepsilon,\varepsilon)
        \end{equation*}
        This gives us
        \begin{equation*}
            \frac{d}{dt}m_X(t)|_{t=0}=\frac{d}{dt}m_Y(t)|_{t=0}
        \end{equation*}
        which is
        \begin{equation*}
            E[X]=E[Y]
        \end{equation*}
        By definition of the expectation,
        \begin{equation*}
            \sum_{x=0}^{n} x\cdot f_X(x)=\sum_{x=0}^{n} x\cdot f_Y(x)
        \end{equation*}
        Now we prove by induction that $\forall n\in \mathbb{N}$, $f_X(x)=f_Y(x)$.

        First when $n=0$ we directly have $f_X(x)=f_Y(x)=1$.Now we want to prove that $f_X(n+1)=f_Y(n+1)$ given that $f_X(n)=f_Y(n)$. This is simple. We first write
        \begin{equation*}
            \begin{split}
            \sum_{x=0}^{n+1} x\cdot f_X(x)&=\sum_{x=0}^{n+1} x\cdot f_Y(x)\\
            \sum_{x=0}^n x\cdot f_X(x)+(n+1)f_X(n+1)&=\sum_{x=0}^n x\cdot f_Y(x) + (n+1)f_Y(n+1)
            \end{split}
        \end{equation*}
        Note that $\sum_{x=0}^n x\cdot f_X(x)=\sum_{x=0}^n x\cdot f_Y(x)$ given that $\forall N\leq n$, $f_X(N)=f_Y(N)$.Thus by cancelling the sums, we have our desired result
        \begin{equation*}
            \begin{split}
                (n+1)f_X(n+1)&=(n+1)f_Y(n+1)\\
                f_X(n+1)&=f_Y(n+1)
            \end{split}
        \end{equation*}
        
        Therefore, by induction we have proved that $f_X(x)=f_Y(x)$ for $x=0,...,n$.
    \end{proof}
\end{exercise}

\begin{exercise}
Sums of Independent Discrete Random Variables
\begin{enumerate}[label=\roman*)]
    \item \begin{proof}
        We first divide $x+y\in ranZ$ into two parts: $x+y=z$ and $x+y\neq z$.
        \begin{equation*}
            \begin{split}
                P[Z=z]&=P[X+Y=z]\\
                &=\sum_{x+y\in ranZ} P[X+Y=z|X=x\land Y=y]\cdot P[X=x\land Y=y]\\
                &=\sum_{x+y\neq z} P[X+Y=z|X=x\land Y=y]\cdot P[X=x\land Y=y]\\
                &+\sum_{x+y=z} P[X+Y=z|X=x\land Y=y]\cdot P[X=x\land Y=y]
            \end{split}
        \end{equation*}
        We note that if $x+y\neq z$, then $P[x+y=z]=0$. Hence
        \begin{equation*}
            \sum_{x+y\neq z} P[X+Y=z|X=x\land Y=y]\cdot P[X=x\land Y=y]=0
        \end{equation*}
        Note that if $x+y=z$, then $P[x+y=z]=1$. Thus
        \begin{equation*}
            \begin{split}
                &\sum_{x+y=z} P[X+Y=z|X=x\land Y=y]\cdot P[X=x\land Y=y]\\
                &=P[X=x\land Y=y]\\
                &=P[X=x]\cdot P[Y=y]
            \end{split}
        \end{equation*}
        Therefore, $P[Z=z]=0+P[X=x]\cdot P[Y=y]=P[X=x]\cdot P[Y=y]$.
    \end{proof}
    \item \begin{proof}
        We denote the parameter of the geometric distribution as $p$, as usual. Now we have $X,Y\sim Geom(p)$. Applying the density function we write $P[X=x]=(1-p)^{x-1}p$ and $P[Y=x]=(1-p)^{x-1}p$. Now the sum of $X$ and $Y$ is $Z=X+Y$. The probability for $Z$ is therefore 
        \begin{equation*}
            \begin{split}
                P[Z=z]&=\sum_{x+y=z}(1-p)^{x-1}\cdot (1-p)^{y-1}p\\
                &=p^2\sum_{x+y=z}(1-p)^{x+y-2}
            \end{split}
        \end{equation*}
        
        We know that $x,y\in \mathbb{N} \backslash {0}$. Thus there are $z-1$ terms in the above sum, resulting in
        \begin{equation*}
            \begin{split}
                P[Z=z]&=p^2(z-1)(1-p)^{z-2}\\
                &=\binom{z-1}{1}p^2(1-p)^{z-2} 
            \end{split}
        \end{equation*}
        which is exactly a Pascal distribution with $r=2$.
    \end{proof}
\end{enumerate}
\end{exercise}
\end{document}

